{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "I0120 17:01:20.458991 104920 file_utils.py:35] PyTorch version 1.4.0+cpu available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm,trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForTokenClassification, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check library version\n",
    "!pip list | grep -E 'transformers|torch|Keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook work with env:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras                2.3.1                 \n",
    "- torch                1.1.0                 \n",
    "- transformers         2.2.0      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, will introduce how to do NER with BERT, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load and preprocess data\n",
    "- Parser data\n",
    "- Make training data\n",
    "- Train model\n",
    "- Evaluate result\n",
    "- Predict result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Update to transformer==2.2.0\n",
    "- When come across OOV,you will find that BERT word piece tokenize method can help a lot\n",
    "- Case model will be litter better than uncase model for English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Also this notebook come with a post [NER with BERT inÂ Action](https://medium.com/@yingbiao/ner-with-bert-in-action-936ff275bc73)**<br>\n",
    "**Feel free to check it, hope that it could help you.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load CSV data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"data/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_address = r\"C:\\Piyush\\NER\\BERT\\Affinity Waters Data\\aw_dataframe.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fillna method can make same sentence with same sentence name\n",
    "df_data = pd.read_csv(data_file_address,sep=\",\",encoding=\"latin1\").fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'text', 'tag', 'pos'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data1 = df_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have a look POS cat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NN', 'CD', 'NNP', 'DT', 'JJ', 'VB', 'VBN', 'IN', 'PRP', 'PRP$',\n",
       "       'RB', 'NNS', 'TO', 'VBG', 'VBP', 'CC', 'MD', 'WDT', 'VBZ', 'VBD',\n",
       "       'WP', 'LS', 'JJS', 'EX', 'WRB', 'RBR', 'SYM', 'FW', 'WP$', 'JJR',\n",
       "       'POS'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.pos.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have a look TAG cat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['others', 'new_address', 'new_men', 'new_tenant', 'move_in_date',\n",
       "       'current_address', 'previous_tenant', 'previous_men',\n",
       "       'move_out_date', 'meter_men', 'meter_reading_val',\n",
       "       'forwarding_men', 'forwarding_address', 'current_men',\n",
       "       'meter_reading_date', 'contact_men', 'contact_val', 'out_men',\n",
       "       'prev_address'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.tag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 8338, 31, 19)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse summary of data\n",
    "df_data['sentence'].nunique(), df_data.text.nunique(), df_data.pos.nunique(), df_data.tag.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "others                35992\n",
       "current_address        4099\n",
       "forwarding_address     1681\n",
       "new_tenant             1661\n",
       "previous_tenant        1023\n",
       "move_in_date            940\n",
       "move_out_date           547\n",
       "new_address             383\n",
       "new_men                 313\n",
       "meter_men               299\n",
       "prev_address            173\n",
       "out_men                 160\n",
       "forwarding_men          149\n",
       "meter_reading_val       139\n",
       "previous_men            123\n",
       "contact_val             116\n",
       "meter_reading_date       71\n",
       "current_men              70\n",
       "contact_men              54\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse the Tag distribution\n",
    "df_data.tag.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain tag\n",
    "As show above, there are two parts for the tag name: \"position\"-\"meaning\"\n",
    "- B: begin, word at the first  position\n",
    "- I: middle, word not at the first position,especially for phase\n",
    "- time: time, meaning time\n",
    "- per: person, meaning people name\n",
    "- geo: geography, meaning location name\n",
    "- O: mean other, set as a default tag\n",
    "<br>......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parser data into document structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"text\"].values.tolist(),\n",
    "                                                           s[\"pos\"].values.tolist(),\n",
    "                                                           s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full document data struce\n",
    "getter = SentenceGetter(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dear',\n",
       " 'Sir/Madam',\n",
       " 'Re:',\n",
       " '6',\n",
       " 'Field',\n",
       " 'view',\n",
       " 'court,',\n",
       " 'Kingsbury,',\n",
       " 'OX0',\n",
       " '0TE.',\n",
       " 'The',\n",
       " 'new',\n",
       " 'tenant,',\n",
       " 'Ms.',\n",
       " 'Naximilian',\n",
       " 'Clajut,',\n",
       " 'have',\n",
       " 'moved',\n",
       " 'into',\n",
       " 'the',\n",
       " 'above',\n",
       " 'property',\n",
       " 'on',\n",
       " 'the',\n",
       " '10\"',\n",
       " 'October',\n",
       " '2017.',\n",
       " 'Herewith',\n",
       " 'we',\n",
       " 'have',\n",
       " 'attached',\n",
       " 'the',\n",
       " 'Tenancy',\n",
       " 'Agreement',\n",
       " 'for',\n",
       " 'your',\n",
       " 'consideration.',\n",
       " 'Kindly',\n",
       " 'amend',\n",
       " 'your',\n",
       " 'records',\n",
       " 'accordingly.',\n",
       " 'For',\n",
       " 'any',\n",
       " 'further',\n",
       " 'information',\n",
       " 'please',\n",
       " 'do',\n",
       " 'not',\n",
       " 'hesitate',\n",
       " 'to',\n",
       " 'contact',\n",
       " 'us.',\n",
       " 'Thank',\n",
       " 'you.',\n",
       " 'Regards,',\n",
       " 'HUB',\n",
       " '1',\n",
       " '%-',\n",
       " '//j///////',\n",
       " '2%',\n",
       " 'APT',\n",
       " 'anie']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sentence data\n",
    "sentences = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NN', 'NN', 'NN', 'CD', 'NN', 'NN', 'NN', 'NNP', 'NN', 'CD', 'DT', 'JJ', 'NN', 'NNP', 'JJ', 'NN', 'VB', 'VBN', 'IN', 'DT', 'IN', 'NN', 'IN', 'DT', 'CD', 'NNP', 'CD', 'NN', 'PRP', 'VB', 'VBN', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', 'RB', 'NN', 'PRP$', 'NNS', 'NN', 'IN', 'DT', 'RB', 'NN', 'NN', 'VB', 'RB', 'NN', 'TO', 'NN', 'NN', 'NN', 'NN', 'NN', 'NN', 'CD', 'NN', 'NN', 'CD', 'NN', 'NN']\n"
     ]
    }
   ],
   "source": [
    "# Get pos data\n",
    "poses = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "print(poses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['others', 'others', 'others', 'new_address', 'new_address', 'new_address', 'new_address', 'new_address', 'new_address', 'new_address', 'others', 'new_men', 'others', 'new_tenant', 'new_tenant', 'new_tenant', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'move_in_date', 'move_in_date', 'move_in_date', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others', 'others']\n"
     ]
    }
   ],
   "source": [
    "# Get tag labels data\n",
    "labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make TAG name into index for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = list(set(df_data[\"tag\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add X  label for word piece support\n",
    "# Add [CLS] and [SEP] as BERT need\n",
    "tags_vals.append('X')\n",
    "tags_vals.append('[CLS]')\n",
    "tags_vals.append('[SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_vals = set(tags_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X',\n",
       " '[CLS]',\n",
       " '[SEP]',\n",
       " 'contact_men',\n",
       " 'contact_val',\n",
       " 'current_address',\n",
       " 'current_men',\n",
       " 'forwarding_address',\n",
       " 'forwarding_men',\n",
       " 'meter_men',\n",
       " 'meter_reading_date',\n",
       " 'meter_reading_val',\n",
       " 'move_in_date',\n",
       " 'move_out_date',\n",
       " 'new_address',\n",
       " 'new_men',\n",
       " 'new_tenant',\n",
       " 'others',\n",
       " 'out_men',\n",
       " 'prev_address',\n",
       " 'previous_men',\n",
       " 'previous_tenant'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a dict for mapping id to tag name\n",
    "#tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "# Recommend to set it by manual define, good for reusing\n",
    "tag2idx={'out_men': 14,\n",
    " 'previous_men': 16,\n",
    " 'meter_men': 0,\n",
    " 'new_tenant': 13,\n",
    " 'new_men': 12,\n",
    " 'move_out_date': 10,\n",
    " 'current_men': 4,\n",
    " 'contact_val': 2,\n",
    " 'forwarding_address': 5,\n",
    " 'forwarding_men':21,\n",
    " 'meter_reading_val': 7,\n",
    " 'prev_address': 15,\n",
    " 'move_in_date': 8,\n",
    " 'new_address': 11,\n",
    " 'current_address': 3,\n",
    " 'meter_reading_date': 6,\n",
    " 'contact_men': 1,\n",
    " 'previous_tenant': 20,\n",
    " 'X':17,\n",
    " 'others': 9,\n",
    " '[CLS]':18,\n",
    " '[SEP]':19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'out_men': 14,\n",
       " 'previous_men': 16,\n",
       " 'meter_men': 0,\n",
       " 'new_tenant': 13,\n",
       " 'new_men': 12,\n",
       " 'move_out_date': 10,\n",
       " 'current_men': 4,\n",
       " 'contact_val': 2,\n",
       " 'forwarding_address': 5,\n",
       " 'forwarding_men': 21,\n",
       " 'meter_reading_val': 7,\n",
       " 'prev_address': 15,\n",
       " 'move_in_date': 8,\n",
       " 'new_address': 11,\n",
       " 'current_address': 3,\n",
       " 'meter_reading_date': 6,\n",
       " 'contact_men': 1,\n",
       " 'previous_tenant': 20,\n",
       " 'X': 17,\n",
       " 'others': 9,\n",
       " '[CLS]': 18,\n",
       " '[SEP]': 19}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping index to name\n",
    "tag2name={tag2idx[key] : key for key in tag2idx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14: 'out_men',\n",
       " 16: 'previous_men',\n",
       " 0: 'meter_men',\n",
       " 13: 'new_tenant',\n",
       " 12: 'new_men',\n",
       " 10: 'move_out_date',\n",
       " 4: 'current_men',\n",
       " 2: 'contact_val',\n",
       " 5: 'forwarding_address',\n",
       " 21: 'forwarding_men',\n",
       " 7: 'meter_reading_val',\n",
       " 15: 'prev_address',\n",
       " 8: 'move_in_date',\n",
       " 11: 'new_address',\n",
       " 3: 'current_address',\n",
       " 6: 'meter_reading_date',\n",
       " 1: 'contact_men',\n",
       " 20: 'previous_tenant',\n",
       " 17: 'X',\n",
       " 9: 'others',\n",
       " 18: '[CLS]',\n",
       " 19: '[SEP]'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make raw data into trainable data for BERT, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set gpu environment\n",
    "- Load tokenizer and tokenize\n",
    "- Set 3 embedding, token embedding, mask word embedding, segmentation embedding\n",
    "- Split data set into train and validate, then send them to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up gpu environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the tokenizer file into local folder first :\n",
    "- [vocab.txt](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual define vocabulary address, if you download the tokenzier file in local\n",
    "# vocab.txt, download from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\n",
    "vocabulary = r\"C:\\Piyush\\NER\\BERT\\cased_L-12_H-768_A-12\\vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Len of the sentence must be not bigger than the training model\n",
    "# See model's 'max_position_embeddings' = 512\n",
    "max_len  = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer, with manual file address or pretrained address\n",
    "tokenizer=BertTokenizer(vocab_file=vocabulary,do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In hunggieface for bert, when come across OOV, will word piece the word\n",
    "- We need to adjust the labels base on the tokenize result, â##abcâ need to set label \"X\" \n",
    "- Need to set \"[CLS]\" at front and \"[SEP]\" at the end, as what the paper do, [BERT indexer should add [CLS] and [SEP] tokens](https://github.com/allenai/allennlp/issues/2141)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.0,len:112\n",
      "texts:[CLS] Dear Sir / Mad ##am Re : 6 Field view court , Kings ##bury , O ##X ##0 0 ##TE . The new tenant , Ms . Na ##xi ##mi ##lian C ##la ##ju ##t , have moved into the above property on the 10 \" October 2017 . Here ##with we have attached the Ten ##ancy Agreement for your consideration . Kind ##ly am ##end your records accordingly . For any further information please do not hesitate to contact us . Thank you . Reg ##ards , H ##U ##B 1 % - / / j / / / / / / / 2 % AP ##T an ##ie [SEP]\n",
      "No.0,len:112\n",
      "lables:[CLS] others others others others X others others new_address new_address new_address new_address new_address new_address X new_address new_address X X new_address X new_address others new_men others others new_tenant new_tenant new_tenant X X X new_tenant X X X new_tenant others others others others others others others others move_in_date move_in_date move_in_date move_in_date move_in_date others X others others others others others X others others others others others others X others X others others others others others others others others others others others others others others others others others others others others X others others X X others others others others others others others others others others others others others others others others X others X [SEP]\n",
      "No.1,len:164\n",
      "texts:[CLS] Dear Sir ##s , R ##E : 25 King Georges Close , Hit ##chin , Hertfordshire , T ##H ##6 3 ##V ##X We are the Managing Agents acting for the landlord of the above property and would advise you that the tenant / s , Mr Mrs E Mad ##d , vacated the property on + 4 / 02 ##26 ##16 . 2 . 5 \\ lo \\ S Met ##er readings taken at the time are as follows ; 01 ##5 ##60 Forward ##ing address 28 ##9 Ke ##ats Way Hit ##chin T ##H ##5 1 ##ET The future account should be in the name of the Land ##lord , Mr Mrs E Q ##ark ##er , and sent to the following address : 4 A ##bb ##is Orchard , I ##ckle ##ford , Hit ##chin , T ##H ##6 3 ##V ##X We trust that this will allow you to update your records correctly . Yours faithful ##ly , [SEP]\n",
      "No.1,len:164\n",
      "lables:[CLS] others others X others others X others current_address current_address current_address current_address current_address current_address X current_address current_address current_address current_address X X current_address X X others others others others others others others others others others others others others others others others others others others others others others others previous_tenant previous_tenant previous_tenant previous_tenant X previous_tenant previous_men others others others others others others others X X others move_out_date move_out_date move_out_date move_out_date move_out_date move_out_date move_out_date meter_men X others others others others others others others others others meter_reading_val X X forwarding_men X forwarding_men forwarding_address X forwarding_address X forwarding_address forwarding_address X forwarding_address X X forwarding_address X others new_men others others others others others others others others others X others new_tenant new_tenant new_tenant new_tenant X X new_tenant others others others others others others others forwarding_address forwarding_address X X forwarding_address forwarding_address forwarding_address X X forwarding_address forwarding_address X forwarding_address forwarding_address X X forwarding_address X X others others others others others others others others others others others others others others others X others [SEP]\n",
      "No.2,len:82\n",
      "texts:[CLS] Dear Sir ##s Re : The Estate \" p ##f the Late Mrs Betty Daphne Child ##s Property : 48 ##4 La ##ns ##bury Drive Hayes VC ##S ##5 9 ##T ##8 We refer to our I ##et : te ##r of 13 September to which we have not received a reply . We en ##c ##lose a further copy in case the original has been mi ##sla ##id and a ##wai ##t the final account . Yours faithful ##ly [SEP]\n",
      "No.2,len:82\n",
      "lables:[CLS] others others X others others others others others others X others others previous_tenant previous_tenant previous_tenant previous_tenant X current_men current_men current_address X current_address X X current_address current_address current_address X X current_address X X others others others others others X others others X others others others others others others others others others others others others others others X X others others others others others others others others others others X X others others X X others others others others others others X [SEP]\n",
      "No.3,len:156\n",
      "texts:[CLS] r ##S ##irs - ' ' Re : 35 ##6 Luton Road . Ha ##rp ##end ##en . Her ##ts B ##M ##6 4 ##EF We write to advise that the / construction of the property is now complete , and change of low ##ners ##hip has taken place . Please note meter reading taken on 15 December 2017 was 82 ##3 ( photo of meter reading attached ) . The new occupants of the property as from 15 December 2017 are : i Ms L T ##tock ##dale and Mrs M Q ##ea ##cock 35 ##6 Luton Road Ha ##rp ##end ##en Hertfordshire B ##M ##6 4 ##EF We would ask that you u fate your records accordingly and a ##wai ##t the final bill . Should you ha { / e any f ##u e que ##ries please contact me on 01 ##9 ##23 270 ##5 ##7 ##6 . Yours faithful ##ly [SEP]\n",
      "No.3,len:156\n",
      "lables:[CLS] others X X others others others others others current_address X current_address current_address current_address current_address X X X current_address current_address X current_address X X current_address X others others others others others others others others others others others others others others others others others others others X X others others others others others others others others others others meter_reading_date meter_reading_date meter_reading_date others meter_reading_val X others others others others others others others others others new_men others others others others others others move_in_date move_in_date move_in_date others others others new_tenant new_tenant new_tenant X X new_tenant new_tenant new_tenant new_tenant X X current_address X current_address current_address current_address X X X current_address current_address X X current_address X others others others others others others others others others others others others X X others others others others others others others others others others others others X others others X others contact_men others others contact_val X X contact_val X X X contact_val others others X [SEP]\n",
      "No.4,len:147\n",
      "texts:[CLS] Dear Sir ##s Re : 9 G ##ull ##iver Close North ##olt VC ##6 6 ##RC Met ##er Reference No : Please note that we are acting as agents for the owner in the letting of the above property . The property had been tenant ##ed by Ms B ##li F ##l - Ba ##ari ##ni , Mrs J ##mel ##da T ##hab ##i We now wish to inform you that the tenant vacated the property on the 20 / 12 / 2017 . The Water Met ##er reading s The Ten ##ants new address : - 206 , C ##lev ##erly Estate W ##orm ##hol ##t Road London X ##23 1 ##UE Please address any further correspondence 1 the owner : - Ms Tour ##en T ##ha ##hi ##ean , 5 Lau ##ght ##on Road North ##olt VC ##6 6 ##MM Yours faithful ##ly [SEP]\n",
      "No.4,len:147\n",
      "lables:[CLS] others others X others others current_address current_address X X current_address current_address X current_address X current_address X others X others others others others others others others others others others others others others others others others others others others others others others others others others others others X others new_tenant new_tenant X new_tenant X new_tenant new_tenant X X new_tenant new_tenant new_tenant X X new_tenant X X others others others others others others others others others out_men others others others others move_out_date move_out_date move_out_date move_out_date move_out_date move_out_date others others meter_men X others others others others X others others others others new_address new_address new_address X X new_address new_address X X X new_address new_address new_address X new_address X others others others others others others others others others others forwarding_address forwarding_address X forwarding_address X X X forwarding_address forwarding_address forwarding_address X X forwarding_address forwarding_address X forwarding_address X forwarding_address X others others X [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = []\n",
    "word_piece_labels = []\n",
    "i_inc = 0\n",
    "for word_list,label in (zip(sentences,labels)):\n",
    "    temp_lable = []\n",
    "    temp_token = []\n",
    "    \n",
    "    # Add [CLS] at the front \n",
    "    temp_lable.append('[CLS]')\n",
    "    temp_token.append('[CLS]')\n",
    "    \n",
    "    for word,lab in zip(word_list,label):\n",
    "        token_list = tokenizer.tokenize(word)\n",
    "        for m,token in enumerate(token_list):\n",
    "            temp_token.append(token)\n",
    "            if m==0:\n",
    "                temp_lable.append(lab)    \n",
    "                \n",
    "            else:\n",
    "                if token.startswith('##'):\n",
    "                    temp_lable.append('X')\n",
    "                else:\n",
    "                    temp_lable.append(lab)\n",
    "                \n",
    "    # Add [SEP] at the end\n",
    "    temp_lable.append('[SEP]')\n",
    "    temp_token.append('[SEP]')\n",
    "    \n",
    "    tokenized_texts.append(temp_token)\n",
    "    word_piece_labels.append(temp_lable)\n",
    "    \n",
    "    if 5 > i_inc:\n",
    "        print(\"No.%d,len:%d\"%(i_inc,len(temp_token)))\n",
    "        print(\"texts:%s\"%(\" \".join(temp_token)))\n",
    "        print(\"No.%d,len:%d\"%(i_inc,len(temp_lable)))\n",
    "        print(\"lables:%s\"%(\" \".join(temp_lable)))\n",
    "    i_inc +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set token embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad or trim the text and label to fit the need for max len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101 12956  2203   120 10779  2312 11336   131   127  3479  2458  2175\n",
      "   117  6560  4109   117   152  3190  1568   121 12880   119  1109  1207\n",
      " 19197   117  6980   119 11896  8745  3080 15647   140  1742  9380  1204\n",
      "   117  1138  1427  1154  1103  1807  2400  1113  1103]\n"
     ]
    }
   ],
   "source": [
    "# Make text token into id\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18  9  9  9  9 17  9  9 11 11 11 11 11 11 17 11 11 17 17 11 17 11  9 12\n",
      "  9  9 13 13 13 17 17 17 13 17 17 17 13  9  9  9  9  9  9  9  9]\n"
     ]
    }
   ],
   "source": [
    "# Make label into id, pad with \"O\" meaning others\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in word_piece_labels],\n",
    "                     maxlen=max_len, value=tag2idx[\"others\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set mask word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine tune of predict, with token mask is 1,pad token is 0\n",
    "attention_masks = [[int(i>0) for i in ii] for ii in input_ids]\n",
    "attention_masks[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set segment embedding(Seem like for sequence tagging task, it's not necessary to make this embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since only one sentence, all the segment set to 0\n",
    "segment_ids = [[0] * len(input_id) for input_id in input_ids]\n",
    "segment_ids[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% for training, 30% for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split all data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks,tr_segs, val_segs = train_test_split(input_ids, tags,attention_masks,segment_ids, \n",
    "                                                            random_state=4, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(373, 161, 373, 161)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_inputs),len(val_inputs),len(tr_segs),len(val_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set data into tensor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not recommend tensor.to(device) at this process, since it will run out of GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pranjan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\pranjan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "C:\\Users\\pranjan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\pranjan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\pranjan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n",
      "C:\\Users\\pranjan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "C:\\Users\\pranjan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "C:\\Users\\pranjan\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs).to(torch.int64)\n",
    "val_inputs = torch.tensor(val_inputs).to(torch.int64)\n",
    "tr_tags = torch.tensor(tr_tags).to(torch.int64)\n",
    "val_tags = torch.tensor(val_tags).to(torch.int64)\n",
    "tr_masks = torch.tensor(tr_masks).to(torch.int64)\n",
    "val_masks = torch.tensor(val_masks).to(torch.int64)\n",
    "tr_segs = torch.tensor(tr_segs).to(torch.int64)\n",
    "val_segs = torch.tensor(val_segs).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Put data into data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch num\n",
    "batch_num = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only set token embedding, attention embedding, no segment embedding\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# Drop last can make batch training better for the last one\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can download the model require files into local folder first\n",
    "- pytorch_model.bin: [pytorch_model.bin](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin)\n",
    "- config.json: [config.json](https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this folder, contain model confg(json) and model weight(bin) files\n",
    "# pytorch_model.bin, download from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin\n",
    "# config.json, downlaod from: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json\n",
    "model_file_address = 'models/bert-base-cased/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0120 17:10:51.770915 104920 configuration_utils.py:182] loading configuration file models/bert-base-cased/config.json\n",
      "I0120 17:10:51.773586 104920 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 22,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0120 17:10:51.777578 104920 modeling_utils.py:403] loading weights file models/bert-base-cased/pytorch_model.bin\n",
      "I0120 17:10:55.107699 104920 modeling_utils.py:480] Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0120 17:10:55.108695 104920 modeling_utils.py:483] Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "# Will load config and weight with from_pretrained()\n",
    "model = BertForTokenClassification.from_pretrained(model_file_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to GPU,if you are using GPU machine\n",
    "# model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add multi GPU support\n",
    "if n_gpu >1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set epoch and grad max num\n",
    "epochs = 10\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cacluate train optimiazaion num\n",
    "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set fine tuning method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True: fine tuning all the layers \n",
    "# False: only fine tuning the classifier layers\n",
    "FULL_FINETUNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_FINETUNING:\n",
    "    # Fine tune model all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # Only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN loop\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 373\n",
      "  Batch size = 32\n",
      "  Num steps = 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.17857461016286502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|ââââââââ                                                                   | 1/10 [01:44<15:36, 104.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.18082312562248923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|ââââââââââââââââ                                                            | 2/10 [03:00<12:45, 95.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.18336716091090982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|âââââââââââââââââââââââ                                                     | 3/10 [04:21<10:38, 91.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.18060225248336792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|âââââââââââââââââââââââââââââââ                                             | 4/10 [05:39<08:43, 87.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.18200187520547348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|ââââââââââââââââââââââââââââââââââââââ                                      | 5/10 [06:54<06:57, 83.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1870760037140413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|ââââââââââââââââââââââââââââââââââââââââââââââ                              | 6/10 [08:07<05:22, 80.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1801903315565803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââ                      | 7/10 [09:21<03:55, 78.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1768119355494326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ               | 8/10 [10:36<02:34, 77.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.18117023868994278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ       | 9/10 [11:47<01:15, 75.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.174408227882602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 10/10 [12:58<00:00, 74.06s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(tr_inputs)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "        attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, scores = outputs[:2]\n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_out_address = 'models/bert_out_model/en09'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dir if not exits\n",
    "if not os.path.exists(bert_out_address):\n",
    "        os.makedirs(bert_out_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a trained model, configuration and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(bert_out_address, \"pytorch_model.bin\")\n",
    "output_config_file = os.path.join(bert_out_address, \"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/bert_out_model/en09\\\\vocab.txt',)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model into file\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(bert_out_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0120 18:18:34.420526 104920 configuration_utils.py:182] loading configuration file models/bert_out_model/en09\\config.json\n",
      "I0120 18:18:34.434521 104920 configuration_utils.py:199] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 22,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0120 18:18:34.440505 104920 modeling_utils.py:403] loading weights file models/bert_out_model/en09\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(bert_out_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to GPU\n",
    "# model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_gpu >1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalue loop\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num examples =161\n",
      "  Batch size = 32\n",
      "f1 socre: 0.537743\n",
      "Accuracy score: 0.895138\n",
      "***** Eval results *****\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            others     0.6166    0.6053    0.6109       380\n",
      "   current_address     0.7125    0.8201    0.7625       139\n",
      "        new_tenant     0.1194    0.2581    0.1633        31\n",
      "           out_men     0.0000    0.0000    0.0000        10\n",
      "      previous_men     0.0000    0.0000    0.0000        10\n",
      "      move_in_date     0.2105    0.3478    0.2623        23\n",
      "           new_men     0.5000    0.3438    0.4074        32\n",
      "         meter_men     0.0000    0.0000    0.0000         6\n",
      "       new_address     0.0000    0.0000    0.0000         5\n",
      "   previous_tenant     0.0909    0.1000    0.0952        30\n",
      "     move_out_date     0.0000    0.0000    0.0000        10\n",
      "      prev_address     0.0000    0.0000    0.0000         5\n",
      "       current_men     0.0000    0.0000    0.0000         6\n",
      " meter_reading_val     0.0000    0.0000    0.0000         1\n",
      "       contact_men     0.0000    0.0000    0.0000         1\n",
      "meter_reading_date     0.0000    0.0000    0.0000         1\n",
      "\n",
      "         micro avg     0.5335    0.5420    0.5377       690\n",
      "         macro avg     0.5226    0.5420    0.5292       690\n",
      "\n",
      "f1 socre: 0.537743\n",
      "Accuracy score: 0.895138\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num examples ={}\".format(len(val_inputs)))\n",
    "print(\"  Batch size = {}\".format(batch_num))\n",
    "for step, batch in enumerate(valid_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    input_ids, input_mask, label_ids = batch\n",
    "    \n",
    "#     if step > 2:\n",
    "#         break\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "        attention_mask=input_mask,)\n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] \n",
    "    \n",
    "    # Get NER predict result\n",
    "    logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    # Get NER true result\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    \n",
    "    \n",
    "    # Only predict the real word, mark=0, will not calculate\n",
    "    input_mask = input_mask.to('cpu').numpy()\n",
    "    \n",
    "    # Compare the valuable predict result\n",
    "    for i,mask in enumerate(input_mask):\n",
    "        # Real one\n",
    "        temp_1 = []\n",
    "        # Predict one\n",
    "        temp_2 = []\n",
    "        \n",
    "        for j, m in enumerate(mask):\n",
    "            # Mark=0, meaning its a pad word, dont compare\n",
    "            if m:\n",
    "                if tag2name[label_ids[i][j]] != \"X\" and tag2name[label_ids[i][j]] != \"[CLS]\" and tag2name[label_ids[i][j]] != \"[SEP]\" : # Exclude the X label\n",
    "                    temp_1.append(tag2name[label_ids[i][j]])\n",
    "                    temp_2.append(tag2name[logits[i][j]])\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "            \n",
    "        y_true.append(temp_1)\n",
    "        y_pred.append(temp_2)\n",
    "\n",
    "        \n",
    "\n",
    "print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "# Get acc , recall, F1 result report\n",
    "report = classification_report(y_true, y_pred,digits=4)\n",
    "\n",
    "# Save the report into file\n",
    "output_eval_file = os.path.join(bert_out_address, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    print(\"***** Eval results *****\")\n",
    "    print(\"\\n%s\"%(report))\n",
    "    print(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\n",
    "    print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "    \n",
    "    writer.write(\"f1 socre:\\n\")\n",
    "    writer.write(str(f1_score(y_true, y_pred)))\n",
    "    writer.write(\"\\n\\nAccuracy score:\\n\")\n",
    "    writer.write(str(accuracy_score(y_true, y_pred)))\n",
    "    writer.write(\"\\n\\n\")  \n",
    "    writer.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
